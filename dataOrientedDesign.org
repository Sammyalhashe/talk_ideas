#+title: practical data oriented design

* caches and memory
- cpu is fast, reading from main memory is slow
- want to try our best to avoid reading from there and having cache misses
- aka, want to avoid evicting cache lines
- this means that if the memory that you are requesting is not already present
  in the cache, the CPU will request the 64 bytes that begin at the cache line
  boundary (the largest address below the one you need that is a multiple of
  64).
- As a rule of thumb, if the processor can't forecast a memory access (and
  prefetch it), the retrieval proces can take ~90ns or ~250 clock cycles (from
  the CPU knowing the address to the CPU receiving the data).
- By constrast, a hit in an L1 cache has a load-use latency of 3-4 clock
  cycles, and a store-reload has a store-forwarding latency of 4-5 clock cycles
  on modern x86 CPUs. Similar on other archs.

* memory layout
- every type has a natural alignment and a size
- CPUs in modern computer hardware perform reads/writes to memory most
  efficiently whe the data is naturall alligned, which generally means that the
  data's memory address is a multiple of the data size.
- the whole purpose of natural alignment is to avoid misaligned access which
  can slow things down significantly. For example, on Linux this is allowed but
  it slows down memory access quite radically, but on other architectures it
  may not actually be allowed and would result in a bus errorthe whole purpose
  of natural alignment is to avoid misaligned access which can slow things down
  significantly. For example, on Linux this is allowed but it slows down memory
  access quite radically, but on other architectures it may not actually be
  allowed and would result in a bus error.
- for structs, it's alignment would be the alignment of the member with the
  largest alignment, and padding is added to accomodate and thus changes the
  size of the structure as a whole. This is, unless you specify bit fields.

ie.
#+begin_src zig
struct {
    a: u32, // alignment 4 bytes
            // <-- 4 bytes of padding needed here
    b: u64, // alignment 8 bytes
    c: u32, // alignment 4 bytes
            // <-- 4 bytes of padding needed here
}; // alignment 8 bytes, size 24 bytes
#+end_src

- However, being alignment aware can help you save space:

#+begin_src zig
struct {
    a: u32, // alignment 4 bytes
    b: u32, // alignment 8 bytes
    c: u64, // alignment 4 bytes
}; // alignment 8 bytes, size 16 bytes
#+end_src

* memory footprint reduction strategies
** Use indexes instead of pointers.
  - If you have a struct that has pointers to objects, pointers can take a lot
    of space 
  - on 64-bit CPUs they are 8 bytes and on 32-bit CPUs they are 4 bytes.
  - Instead you can replace them with integers and not heap allocate the
    objects they refer to and just have them in some globally accessible list
    you can save space because a) integers are generally smaller than pointers,
     and this means that the alignment will also be generally smaller.
  - Integers are 4 bytes no matter the architecture.
** Move state to multiple instantiations of collections. (Store booleans out-of-band).
 ie.
 #+begin_src zig
   // instead of this:
   const Monster = struct {
        anim: *Animation, // 8 bytes
        hp: u32,          // 4 bytes
        y: u32,           // 4 bytes
        alive: bool       // 1 byte
   }; // alignment 8 bytes, size 24 bytes
 
   var Monsters: ArrayList(Monster) = .{}; // 63 wasted bits per element
   
   // consider something like this:
   const Monster = struct {
        anim: *Animation, // 8 bytes
        hp: u32,          // 4 bytes
        y: u32,           // 4 bytes
   }; // alignment 8 bytes, size 16 bytes
    
   // now we waste 0 bits per element
   var AliveMonsters: ArrayList(Monster) = .{};
   var DeadMonsters: ArrayList(Monster) = .{};

   // additionally you can do the index trick as well!
 #+end_src

   - Another thing to notice is that your array reads are faster now because
     now you don't need to check the ~alive~ flag, which could have evicted a
     cache line, so you're not paying for that anymore.
 
** Use Struct of Arrays instead of Array of Structs to eliminate padding

   consider:
   #+begin_src zig
      const MonsterMisaligned = struct {
          anim: *Animation, // alignment 8 bytes, size 8 bytes
          kind: Kind,       // enums are 1 byte, 1 byte alignment obv.

          const Kind = enum {
             snake, bat, wolf
          };
      }; // alignment 8 bytes

      // each element pays 7 bytes of padding
      var Monsters: ArrayList(MonsterMisaligned) = .{};

      // But if instead of this you have:

      const Monster = struct {
          animations: ArrayList(*Animation), // Each element has no padding
          kinds: ArrayList(*Kind),           // same here

          const Kind = enum {
             snake, bat, wolf
          };
      };

      // where each element in animations corresponds to an element in kinds
      // and vice-versa

      // each element pays 7 bytes of padding
      var Monsters: ArrayList(Monster) = .{};

      // In zig you could have also used the previous `Monster` declaration
      // with something called a `MultiArrayList`.
      var Monsters = ArrayList(MonsterMisaligned) = .{};
   #+end_src


** Store sparse data in hash maps.
   - basically, data that may not be associated with every structure should be
     stored in hash maps to avoid allocating space for data that may or may not
     be used (especially if that use is rare).

** "Encode" data using enums and use indices to point to the extra data that may be associated with the encoded type out of band. Use instead of OOP/polymorphism.


* Other things to consider
** cache associativity
   - two ways of doing cache associativity: direct mapped and fully associative.
   - fully associative is hard to build (require more hardware) and means any data can go anywhere
   - each data has a direct mapping to a place in a cache but this means you'll have more cache misses and you can't always keep things in cache when you need to.
   - modern machines use something called an n-way associative cache where the cache is divided into n sets, and each set will be direct mapped.
** temporal cache coherance
   - if you access data you should process it as soon as possible.
   - there is no guarantee it won't be evicted from the cache line by the time
     you actually process it (if you leave it hanging around for a while).
** avoiding cache misses
   - if you find you use two pieces of data together (maybe they're in seperate
     array idk), then put them in the same array so they are close enough.
